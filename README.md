# Detecting Anti-Money Laundering (AML) Activities Using Machine Learning

This project presents a machine learning–based Anti-Money Laundering (AML) detection system built on large-scale financial transaction data generated by IBM.  
The work focuses on identifying illicit financial transactions hidden within massive volumes of legitimate activity by explicitly addressing extreme class imbalance, applying robust feature engineering, and evaluating multiple machine learning models under realistic deployment conditions.

The overall experimental design mirrors real-world AML scenarios by training models on data with a higher proportion of illicit activity and evaluating them on data with significantly lower illicit ratios.

---

## About the Dataset

### IBM Synthetic AML Dataset

The dataset used in this project is a synthetic financial transaction dataset created by IBM to support Anti-Money Laundering research while avoiding challenges associated with real financial data, such as privacy constraints and unreliable labeling.
- [HI-Small_Trans.csv: (High Illicit Activity)](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/data?select=HI-Small_Trans.csv) $\rightarrow$ Training/Validation.
- [LI-Small_Trans.csv: (Low Illicit Activity)](https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml/data?select=LI-Small_Trans.csv) $\rightarrow$ Testing.


Dataset link:  
https://www.kaggle.com/datasets/ealtman2019/ibm-transactions-for-anti-money-laundering-aml

Key characteristics of the dataset include:

- A complete virtual financial ecosystem involving individuals, businesses, and banks  
- Explicit modeling of the entire money laundering lifecycle  
- Accurate ground-truth labels for every transaction (Laundering or Legitimate)  
- Full visibility across all virtual banks, enabling holistic transaction analysis  

### Datasets Used

- HI-Small_Trans.csv (High Illicit Activity)  
  Used for model training and validation  

- LI-Small_Trans.csv (Low Illicit Activity)  
  Used exclusively for final testing and generalization analysis  

This train–test separation closely simulates real-world AML deployment, where historical data may contain a higher proportion of known illicit cases compared to future operational data.

---

## Models Overview

This project evaluates multiple classical and ensemble-based machine learning models for AML detection, with particular emphasis on how class imbalance handling strategies affect performance.

The following sections focus on a detailed comparison between two imbalance mitigation approaches:

1. Undersampling only  
2. Undersampling combined with SMOTETomek  

---

## Model Performance Comparison: Undersampling vs Undersampling + SMOTETomek

This comparison is based on two separate notebooks that implement the same end-to-end pipeline:

- Identical exploratory data analysis  
- Identical feature engineering and preprocessing  
- Identical train–test splits  
- Identical machine learning models and evaluation metrics  

The only controlled variable between the experiments is the class imbalance handling strategy, ensuring a methodologically sound comparison.

---

## Background: Class Imbalance in AML Detection

AML datasets are inherently imbalanced, with laundering transactions representing a very small fraction of total financial activity. This imbalance introduces several challenges:

- Models become biased toward the majority (non-laundering) class  
- Minority-class recall is often low, increasing the risk of missed laundering cases  
- Decision boundaries near overlapping regions are poorly learned  

Effective imbalance handling is therefore critical for building reliable AML detection systems.

---

## Imbalance Handling Strategies Compared

### Undersampling Only

- Randomly reduces the majority class to match the minority class  
- Removes class bias and improves baseline recall  
- Discards a large portion of legitimate transactions  
- May result in loss of important transactional patterns  

### Undersampling Combined with SMOTETomek

- Applies undersampling to reduce majority dominance  
- Uses SMOTE to generate synthetic minority-class samples  
- Removes Tomek links to clean overlapping decision boundaries  
- Preserves decision-critical information while improving class separability  

---

## High-Level Performance Comparison

| Aspect | Undersampling Only | Undersampling + SMOTETomek |
|------|------------------|----------------------------|
| Data Distribution | Balanced via reduction | Balanced via reduction and synthesis |
| Minority Representation | Limited to original samples | Enhanced through synthetic samples |
| Decision Boundary Quality | Coarse | Refined and cleaner |
| Minority Recall | Moderate to high | Consistently high |
| Precision | Higher | Slightly reduced |
| Model Generalization | Moderate | Improved |
| False Negatives | Higher | Significantly reduced |

---

## Model-Wise Comparative Analysis

### Logistic Regression

**Undersampling Only**
- Serves as a stable baseline model  
- Improves minority-class recall compared to raw data  
- Limited by linear decision boundaries  
- Unable to capture complex transactional relationships  

**Undersampling + SMOTETomek**
- Marginal improvement in minority recall  
- Minimal overall performance gain  
- Synthetic samples provide limited benefit due to model simplicity  

Inference:  
Logistic Regression benefits only marginally from SMOTETomek and is insufficient as a standalone AML model where non-linear patterns dominate.

---

### Random Forest Classifier

**Undersampling Only**
- Strong overall accuracy  
- Good laundering-class recall  
- Effectively captures non-linear interactions  
- Feature importance highlights transaction amounts, timing, and currency behavior  

**Undersampling + SMOTETomek**
- Improved recall for laundering transactions  
- Better classification of borderline and ambiguous cases  
- Slight reduction in precision due to increased sensitivity  
- Improved robustness across validation splits  

Inference:  
SMOTETomek meaningfully enhances Random Forest performance by improving minority-class learning while maintaining overall stability.

---

### XGBoost Classifier

**Undersampling Only**
- High accuracy and strong ROC-AUC score  
- Excellent performance on structured financial data  
- Learns complex feature interactions efficiently  

**Undersampling + SMOTETomek**
- Further improvement in laundering recall  
- More stable and smoother ROC curves  
- Better generalization on low-illicit test data  
- Maintains a strong precision–recall balance  

Inference:  
XGBoost benefits the most from SMOTETomek, as gradient boosting effectively leverages synthetic samples while controlling overfitting.

---

### Stacking Classifier

**Undersampling Only**
- Combines strengths of multiple base learners  
- Achieves balanced precision and recall  
- Performs well on higher-level transactional patterns  

**Undersampling + SMOTETomek**
- Increased sensitivity to minority-class instances  
- Slight rise in false positives  
- More consistent predictions in overlapping decision regions  

Inference:  
SMOTETomek improves the stacking model’s ability to detect subtle laundering behaviors near class boundaries.

---

## Metric-Level Trends Observed

| Metric | Observed Trend |
|------|---------------|
| Accuracy | Remains largely stable across both strategies |
| Recall (Laundering) | Consistently higher with SMOTETomek |
| Precision | Slightly lower with SMOTETomek |
| F1-Score | Improves due to higher recall |
| ROC-AUC | More stable and reliable with SMOTETomek |
| False Negatives | Significantly reduced with SMOTETomek |

---

## Practical Implications for AML Systems

- False negatives are more costly than false positives in AML contexts  
- Higher recall directly reduces the risk of undetected laundering  
- Slightly reduced precision is acceptable when combined with downstream human review  
- Ensemble models benefit most from advanced resampling techniques  

---

## Final Conclusion

Undersampling alone provides a strong baseline by removing class bias but at the cost of discarding valuable information.  
Combining undersampling with SMOTETomek consistently leads to better minority-class learning, cleaner decision boundaries, and improved generalization.

Ensemble-based models, particularly Random Forest and XGBoost, show the largest performance gains under SMOTETomek.  
For real-world AML deployment, undersampling combined with SMOTETomek and ensemble classifiers offers the most balanced and reliable approach, prioritizing laundering detection while maintaining acceptable precision.

This structured comparison highlights the critical role of imbalance-handling strategies in building effective and deployment-ready AML detection systems.
